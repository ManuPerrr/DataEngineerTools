{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet\n",
    "\n",
    "### Description du projet\n",
    "    - Création d'une application Web vous basant sur le package Flask.\n",
    "    - Récupération des données sur le web scrapées grâce à ce que vous aurez appris durant ce cours.\n",
    "    - Affichage des données de façon optimale, moteur de recherche, graphiques, etc. \n",
    "    - Utilisation d'une ou plusieurs bases de données abordées dans ce cours. \n",
    "    - Création d'une documentation technique et fonctionnelle.  \n",
    "    \n",
    "### Description \n",
    "    -Site utilisé : https://www.allocine.fr/\n",
    "    -robot.txt : https://www.allocine.fr/robots.txt\n",
    "    \n",
    "    -Récupération d'informations possibles : \n",
    "        (Scrap#1) Page : Top film (10x30 films par pages) : \n",
    "            Date/Durée/Genre/Note presse/Note spectateur/Nombre de votes/Synopsis/Budget/Langue/Nombre Récompense/Distributeur/Casting Principal/Réalisateur/Top critique Spectateur\n",
    "            (Scrap#2) Page Casting pour avoir la liste complète\n",
    "            (Scrap#3) Page des récompenses pour avoir le détail\n",
    "        (Scrap#1) Page : Films à l'affiche : Même données\n",
    "            (Scrap#4) Page Du box office pour relier à l'affiche \n",
    "        (Scrap#1) Page : Tous les films trié par popularité/Note presse/Note pop (15 films par page)\n",
    "    -Possible utilisation des données offrant une plus value avec ou sans graphe\n",
    "        -Histogramme des moyennes de note(pop ou presse)/Nombre de critique des films d'un réalisateur par année de sortie\n",
    "        -Diagramme circulaire des genres de film à l'affiche\n",
    "        -Graphe de l'évolution de box office d'un film à l'affiche sur la dernière semaine (à vérifier)\n",
    "        -Liste de film par paire de membre de casting \n",
    "        -Autres à définir\n",
    "\n",
    "    -Scraping : Utilisation de Scrapy préféré\n",
    "\n",
    "### Objectif :\n",
    "    -Choisir ce que l'on veut récupérer comme données\n",
    "    -Prendre en compte le robot.txt\n",
    "    -Récupération des données\n",
    "    -En faire une base de donnée \n",
    "    -Nettoyer la base de donnée\n",
    "    -Mettre en relation les bases de données (si plusieurs)\n",
    "    -Manipuler les données\n",
    "    -Faire la page web \n",
    "    -Faire la page web de manière interactive\n",
    "    \n",
    "\n",
    "### Questions :\n",
    "    -Est-ce que l'on peut utiliser Dash pour la page Flask ? \n",
    "    -Comment interpréter le robot.txt\n",
    "    -La documentation c'est un User guide + Developpeur Guide ou c'est un rapport des étapes/difficultés etc...\n",
    "    -Plus value nécessaire ?\n",
    "    \n",
    "### Réponse :\n",
    "\n",
    "Par rapport à la valeur ajouté il faudrait ajouter quelque chose qui existe ailleur\n",
    "Faudrait rassembler les données de allociné et les lier aux données d’autres sites internets \n",
    "Exemple : \n",
    "Si on récupère le casting d’allociné on peut ajouter un scrap twitter pour associer au nombre de follower, on peut alors voir les films avec le plus de follower d’acteur et essayer de faire des liaisons théoriques\n",
    "Ca peut être autre chose et pas que des réseaux sociaux \n",
    "=> faut ajouter une dimension/introduire une notion qui n’existe pas ailleur\n",
    "=> Un peu du bonus mais très cool à faire ne plus\n",
    "\n",
    "Comment relier le nom des acteurs aux twitter ?\n",
    "\t=> Moteur de recherche, on récupère le pseudo, on va sur la page twitter puis les foll\n",
    "\t=> Rechercher un site qui recense les page twitter des personnalités \n",
    "\t=> \n",
    "\n",
    "Est-ce que l’on peut utiliser dash ? \n",
    "=> Attendre les conseilles de Raphaël \n",
    "\n",
    "\n",
    "Comment interpréter le robot.txt \n",
    "=> Rien n’est interdit, c’est juste que le site n’aimerait pas trop mais voilà on fait pas de bénéfices derrières donc on peut y aller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Récupération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (0.2.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (19.1.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (3.2.1)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (4.6.1)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (1.0.3)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (20.3.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from scrapy) (5.2.0)\n",
      "Requirement already satisfied: six>=1.6.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.14.3)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: pyasn1 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (20.3.0)\n",
      "Requirement already satisfied: pyasn1-modules in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: Automat>=0.3.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (17.5.0)\n",
      "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (2.0.2)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (20.0.1)\n",
      "Requirement already satisfied: constantly>=15.1 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: setuptools in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from zope.interface>=4.1.3->scrapy) (50.3.2)\n",
      "Requirement already satisfied: pycparser in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
      "Requirement already satisfied: idna>=2.5 in /root/.local/share/virtualenvs/code-AFz48Hjb/lib/python3.8/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n"
     ]
    }
   ],
   "source": [
    "#Vérification de l'installation de scrapy et importation\n",
    "!pip install scrapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 16:19:15 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\n",
      "2020-12-29 16:19:15 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.6 (default, Nov 18 2020, 13:49:49) - [GCC 8.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.2.1, Platform Linux-4.19.128-microsoft-standard-x86_64-with-glibc2.2.5\n",
      "2020-12-29 16:19:15 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-12-29 16:19:15 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'SPIDER_LOADER_WARN_ONLY': True}\n",
      "2020-12-29 16:19:15 [scrapy.extensions.telnet] INFO: Telnet Password: 385e5d61ffa58ff0\n",
      "2020-12-29 16:19:15 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-12-29 16:19:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-12-29 16:19:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-12-29 16:19:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-12-29 16:19:15 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-12-29 16:19:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-12-29 16:19:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-12-29 16:19:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.allocine.fr/film/meilleurs/> (referer: None)\n",
      "2020-12-29 16:19:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.allocine.fr/film/meilleurs/>\n",
      "{'Titre': 'Forrest Gump'}\n",
      "2020-12-29 16:19:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.allocine.fr/film/meilleurs/>\n",
      "{'Titre': 'La Ligne verte'}\n",
      "2020-12-29 16:19:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.allocine.fr/film/meilleurs/>\n",
      "{'Titre': 'La Liste de Schindler'}\n",
      "2020-12-29 16:19:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.allocine.fr/film/meilleurs/>\n",
      "{'Titre': 'Le Parrain'}\n",
      "2020-12-29 16:19:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.allocine.fr/film/meilleurs/>\n",
      "{'Titre': '12 hommes en colère'}\n",
      "2020-12-29 16:19:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.allocine.fr/film/meilleurs/>\n",
      "{'Titre': 'Les Evadés'}\n",
      "2020-12-29 16:19:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.allocine.fr/film/meilleurs/>\n",
      "{'Titre': 'Le Seigneur des anneaux : le retour du roi'}\n",
      "2020-12-29 16:19:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.allocine.fr/film/meilleurs/>\n",
      "{'Titre': 'Your Name'}\n",
      "2020-12-29 16:19:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.allocine.fr/film/meilleurs/>\n",
      "{'Titre': 'The Dark Knight, Le Chevalier Noir'}\n",
      "2020-12-29 16:19:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.allocine.fr/film/meilleurs/>\n",
      "{'Titre': \"Vol au-dessus d'un nid de coucou\"}\n",
      "2020-12-29 16:19:15 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-12-29 16:19:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 230,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 60513,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.266704,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 12, 29, 16, 19, 15, 499160),\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/DEBUG': 11,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 56344576,\n",
      " 'memusage/startup': 56344576,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2020, 12, 29, 16, 19, 15, 232456)}\n",
      "2020-12-29 16:19:15 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider film_title.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projet stade\n",
    "\n",
    "\n",
    "Prochaine étape : \n",
    " #####  - ✓ Faire la commande pour récupérer la liste des titres dans la page meilleur film\n",
    "Dans le shell : <br>\n",
    "scrapy shell \"https://www.allocine.fr/film/meilleurs/\" <br>\n",
    "On a la liste des titres en marquant  <br>\n",
    "[response.css(\"#content-layout .mdl\")[i].css(\"a::text\").extract_first() for i in range (10)] <br>\n",
    " #####  - ✓ Faire la class pour récupérer les titres + lien de la page des films \n",
    " Classe faite => Projet/newscrawler/newscrawler/spiders/besttitle <br>\n",
    " Activation : cmd dans la page projet spider puis : scrapy crawl besttitle <br>\n",
    " Activation + save : cmd dans la page projet puis : scrapy crawl besttitle -o titlelink.csv <br>\n",
    " \n",
    " #####  - ✓ Faire  la commande pour scrap des informations recherchés sur chaque lien\n",
    " Check dans la classe <br>\n",
    " #####  - Faire la class de ce nouveau scrap \n",
    " Classe faite => Projet/newscrawler/newscrawler/spiders/infomovie <br>\n",
    " Information données : Date Durée Genre Note spectateur Nombre de votes Synopsis Casting Principal Réalisateur Nbre critique Note min  Note max Note presse Nombre Récompense Budget <br>\n",
    " <br>\n",
    " Classe à tester sur la page de Forest Gump voir si on a tout, puis sur une autre page \n",
    " <br> Faire en sorte de pouvoir ajouter la liste des liens dans la class à partir des liens du fichier de la classe précedente\n",
    " <br> Enregistrer le résultat dans un fichier \n",
    " <br> Traiter l'ensemble des dictionnaires afin d'éliminer les éléments non voulus,(garder le titre pour pouvoir faire la liaison entre les bases de donnée) <br>\n",
    " #####  - Regarder comment récupérer les informations des scraps + traitement\n",
    " On peut enregistrer les résultats sur un fichier csv => puis pandas <br>\n",
    " #####  - Scraper la fonction recherche du boxoffice mojo\n",
    " #####  - Mise en place dans une base de donnée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprendre la version 2 du parse et ajouter parse catégorie qu'on applique pour chaque lien, vérifier si on peut bien récupérer tous les liens avec un simple value, sinon repartir dans l'idée du second parse de base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
